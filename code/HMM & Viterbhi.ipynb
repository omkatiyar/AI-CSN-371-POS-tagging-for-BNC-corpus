{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLease this time, before Anyone make any changes, please let me know, \n",
    "# Since Mam Has given us one more chance, I don't want to miss it\n",
    "# thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to parse the files\n",
    "import xml.etree.ElementTree as xmltree\n",
    "\n",
    "# for searching the folders\n",
    "from os.path import isdir\n",
    "from os.path import isfile\n",
    "from os import mkdir\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will directly dump the word_tag pairs from give file in the their corresponding word_tag files\n",
    "# in sentence format\n",
    "\n",
    "# so the format is \n",
    "# every line of dest file will be a sentence\n",
    "# we will only consider word tags this time.\n",
    "# and sentences will be stored as word_tag pairs\n",
    "\n",
    "def parseFile_mk(source,dest) :\n",
    "    \n",
    "    # xmlfile is the file to be handled\n",
    "    xmlfile = open (source, \"r\",encoding= \"utf-8\")\n",
    "    # we get the text into a variable\n",
    "    xmlfiletext = xmlfile.read();\n",
    "    #close xmlfile\n",
    "    xmlfile.close()\n",
    "    \n",
    "    # make am xml tree from that text\n",
    "    tree = xmltree.fromstring(xmlfiletext)\n",
    "    \n",
    "    # make list of sentences\n",
    "    slist = tree.iter('s')\n",
    "    \n",
    "    #open dest file with **overwriting** mode\n",
    "    dfile = open (dest[:-4]+\"_sentence.txt\", \"w+\", encoding = \"utf-8\")\n",
    "    \n",
    "    \n",
    "    # put the data in the sfile\n",
    "    for sentence in slist :\n",
    "        # declare a line variable to store the sentence\n",
    "        line = \"\"\n",
    "        # iterate over every variable in sentence\n",
    "        for elem in sentence :\n",
    "            #make sure we only process words for a sentence\n",
    "            if elem.tag != 'w' :\n",
    "                continue \n",
    "            # fetch pos tag of this elem\n",
    "            pos = elem.get('c5')\n",
    "            # if the child doesnot contains c5 then it is of no use for us\n",
    "            if pos is None :\n",
    "                continue\n",
    "                \n",
    "            # upper case all pos tags    \n",
    "            pos = pos.upper()   \n",
    "            \n",
    "            # some where the authors were confused and they assigned two tags\n",
    "            # in such scenarios we are deciding to go with first tag and ignore the second\n",
    "#             pos = pos[:3]\n",
    "            \n",
    "            # if I am here it  means pos has c5 tag available so we need to find the text of it.\n",
    "            txt = elem.text\n",
    "            if txt is None : \n",
    "                continue\n",
    "            txt = txt.strip()\n",
    "            # append in the sentence this word\n",
    "            line =   line +      txt + \"_\"+ pos        +\" \"  # extra space added between terms \n",
    "                                                             # to just make more clear\n",
    "        # for word loop finshes\n",
    "        # write the sentence to the file\n",
    "        dfile.write(line+\"\\n\")\t\n",
    "    # for sentence loop finished \n",
    "    #close file\n",
    "    dfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "# this will fetch all the files and generate corresponding files\n",
    "# this takes source folder and \n",
    "def makeSentences_mk(source, dest) :\n",
    "\n",
    "    #check if the source is a file\n",
    "    if isfile(source):\n",
    "        # if yes, then process the file \n",
    "        # if destination is not there , the function will create it, and if it is there, it will overrite it.\n",
    "        parseFile_mk(source,dest)\n",
    "    else:\n",
    "        # so the source is a folder, we need to make corresponding folder in the dest \n",
    "        #check if dest has the folder\n",
    "        if not isdir(dest):\n",
    "            mkdir(dest)\n",
    "        \n",
    "        # now enumerate all the items in the source directory    \n",
    "        for item in listdir(source):\n",
    "            makeSentences_mk(source+'/'+item,dest+'/'+item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Uncomment & Run below Cell to Process Files, \n",
    "## I do processed my files so I have commented this to avoid repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this segment is used to make the files process all togethor in their own files\n",
    "\n",
    "source = \"given_data\"\n",
    "dest = \"processed_data\"\n",
    "makeSentences_mk(source, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Function to Combine all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collector function, this will dump all sentences into the destination file\n",
    "def dump_mk(source,dest):\n",
    "\n",
    "    sfile = open(source, \"r\", encoding= \"utf-8\")\n",
    "    dfile = open(dest, \"a\", encoding= \"utf-8\")\n",
    "\n",
    "    # read entire sfile line by line, yes this is the key\n",
    "    # Possible Error Point : in our Previous Implementation I was reading It all at same time, this was problem ,\n",
    "        # whole file became a single sentence\n",
    "    # and then dump that  into the dfile as itis.\n",
    "    for line in sfile :\n",
    "        dfile.write(line + \"\\n\")\n",
    "\n",
    "    sfile.close()\n",
    "    dfile.close()\n",
    "\n",
    "# this iterates and finds all the files and stores them into a single file \n",
    "# using dumping function\n",
    "def combine_mk(source, dest) :\n",
    "\n",
    "    #check if the source is a file\n",
    "    if isfile(source):\n",
    "        # if yes, then process the file \n",
    "        # if destination is not there , the function will create it, and if it is there, it will overrite it.\n",
    "        dump_mk(source,dest)\n",
    "    else:\n",
    "        # so the source is a folder, \n",
    "        # now enumerate all the items in the source directory  \n",
    "        # and take appropriate actions  \n",
    "        for item in listdir(source):\n",
    "            combine_mk(source+'/'+item,dest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Uncomment Below TWO Cells to Combine the Test & Train Data\n",
    "## I did it thus I have left them commented\n",
    "\n",
    "If you need anyhelp Sritam, just whatsapp me .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to combine all the processed files for both train & test data sets \n",
    "# together one to each other in their respective files\n",
    "\n",
    "# Combine Test Data\n",
    "s=\"processed_data/test_corpus\"\n",
    "d = \"processed_data/test_combine.txt\"\n",
    "dfile = open(d,\"w+\", encoding=\"utf-8\")\n",
    "dfile.close()\n",
    "\n",
    "combine_mk(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine Train Data\n",
    "s=\"processed_data/train_corpus\"\n",
    "d = \"processed_data/train_combine.txt\"\n",
    "dfile = open(d,\"w+\", encoding=\"utf-8\")\n",
    "dfile.close()\n",
    "\n",
    "\n",
    "combine_mk(s,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Emission Probabilities\n",
    "\n",
    "I saw many videos on youtube with many examples, now I think this is first step we should do. \n",
    "And if we can do this nicely, this will give us COnfidence as well :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does Emission Probability means?\n",
    "Make this thing clear, so that implementation is not wrong.\n",
    "\n",
    "Hidden states are tags \n",
    "And observations are words... \n",
    "\n",
    "So emission probability means, for word we calculate the chance that given tag, what is probability this word will occur \n",
    "\n",
    "Emission P of word w with tag t = Count of word_tag pairs with word = w and tag = t divided by count of word_tag pairs with tag equal to t no matter what word is....\n",
    "\n",
    "### So far so good ... \n",
    "\n",
    "Obviously this makes things clear.\n",
    "\n",
    "I will make a dictionary counting appearnace of every tag , this will give me count of word_tag pairs with tag t for every tag t\n",
    "\n",
    "Or better make dictionary of dictionary\n",
    "Dict with keys as tags, and second dictionary with keys as words ? how's this ... seems fine right... this will give us all the data we need to go \n",
    "\n",
    "Finally to save it I will pickle the struct, and will see if I need to do anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the emission probabilty calculator functtion\n",
    "\n",
    "def emission_mk():\n",
    "    # obviosly source file is \n",
    "    data = open(\"processed_data_sentence/train_sentence_combine_.txt\", \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    print(\"file successfully loaded\")\n",
    "    # make dictonary for tags as keys\n",
    "    # this will have tag as keys and values as their count\n",
    "    tag_dict = {}\n",
    "    \n",
    "    # make dictionary for tags and keys \n",
    "    # values will be another dictionary for words\n",
    "    tag_dict_dict = {}\n",
    "    \n",
    "    # count number of sentences\n",
    "    count = 0\n",
    "    # now read every line one by one of this file\n",
    "    for line in data :\n",
    "        \n",
    "        # if line is a new line then skip this line\n",
    "        if len(line) < 2 :\n",
    "            continue\n",
    "            \n",
    "        # split line into word_tag pairs with split\n",
    "        pairs = line.split(\" \")\n",
    "        \n",
    "        \n",
    "        # iterate over pairs\n",
    "        for pair in pairs :\n",
    "            # split the give pair \n",
    "            try :\n",
    "                [word,tag] = pair.split(\"_\")\n",
    "            except : \n",
    "                print(pair)\n",
    "                continue\n",
    "                    \n",
    "            [word,tag] = pair.split(\"_\")\n",
    "            # increase frequency for this tag in tag dictinary\n",
    "            tag_dict[tag] = tag_dict.get(tag,0) + 1\n",
    "            \n",
    "            # obtain the dictionary with given tag as key\n",
    "            # in case the tag is not there, then the default value must be returned : empty dictionary\n",
    "            temp_word_dict = tag_dict_dict.get(tag, {}) \n",
    "            # now increase the count in the word dictinary for corresponding word\n",
    "            temp_word_dict[word] = temp_word_dict.get(word,0) + 1\n",
    "            # now assign this dictionary to the system\n",
    "            tag_dict_dict[tag] = temp_word_dict\n",
    "            \n",
    "            # all dictionaries filled properly\n",
    "        # pairs in sentence processed\n",
    "        count = count +1\n",
    "        print(\"sentence number :\", count, \" processed\")\n",
    "    # all the lines are now processed\n",
    "    \n",
    "    # now all dictionaries are properly filled with values\n",
    "    \n",
    "    #pickle the first dictionary\n",
    "    tag_dict_picklefile = open(\"pickle/tagscount\",\"wb+\")\n",
    "    pickle.dump(tag_dict, tag_dict_picklefile)\n",
    "    tag_dict_picklefile.close()\n",
    "    \n",
    "    # count emission probabilties\n",
    "    # to do so we divide the word counts given tag with the count of the tag\n",
    "    # iterate over all the tags in the system\n",
    "    for tag in tag_dict :\n",
    "        \n",
    "        if tag_dict[tag] == 0:\n",
    "            continue\n",
    "        # for every word in the corresponding dictionary of the word\n",
    "        for word in tag_dict_dict[tag]:\n",
    "            tag_dict_dict[tag][word] = (1.0*tag_dict_dict[tag][word]) / tag_dict[tag]\n",
    "        # all words for this tag settled\n",
    "    # all tags settled\n",
    "    \n",
    "    #pickle the second dictionary\n",
    "    emission_pickle = open (\"pickle/emission\", \"wb+\")\n",
    "    pickle.dump(tag_dict_dict,emission_pickle)\n",
    "    emission_pickle.close()\n",
    "    \n",
    "    # function complete        \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Please Uncomment Below Code to make the emission probability pickle object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the above function for generating the files\n",
    "emission_mk()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transission Probabilties\n",
    "\n",
    "what are they, first of all \n",
    "\n",
    "Basically they predict that after the apearance of certain tag t1 (say), what is the probability that next tag will be t2(saY). \n",
    "\n",
    "\n",
    "T2 may or may not be equal to T1.\n",
    "\n",
    "## Next Obvious Question, How to get these values\n",
    "\n",
    "I think, I will iterate sentence wise, with each sentence begining with a special tag, called start, we will store the count of sentences (something around 4lac 32 thousands or 4lac 83 thousands. I don't remember exact values.\n",
    "\n",
    "I got this count during emission probabilty.\n",
    "Unfortunately I didnot saved this data.\n",
    "\n",
    "Never mind.\n",
    "Let's get back to task!\n",
    "\n",
    "So Transission P (t2) with t1 = Num of times t2 occurs after occurance of t1 / num of times t1 occurs in the data set. .... \n",
    "\n",
    "the denominator is simply the tag count pickle, which is to be unloaded. :) \n",
    "\n",
    "for numerator I need to store the number of times, t2 occurs after t1 .\n",
    "\n",
    "Example : Mohit (Noun) is(verb) a(adj) Boy(noun). So we will make a new tag, \"START\"\n",
    "New Sentence will be : \"\"(START) Mohit (Noun) is(verb) a(adj) Boy(noun). \n",
    "\n",
    "let's start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find transmission probabilities\n",
    "def transmission_mk():\n",
    "    \n",
    "    # open the file\n",
    "    data = open(\"processed_data/train_combine.txt\", \"r\", encoding=\"utf-8\")\n",
    "    print(\"file successfully loaded\")\n",
    "    \n",
    "    # create dictionary for storing the tag pairs\n",
    "    # structure tag_dict[t1][t2] = number of times t2 appears after the occurence of t1\n",
    "    tag_dict = {}\n",
    "    \n",
    "    # counter \n",
    "    count = 0\n",
    "    \n",
    "    #iterate over every sentence in the train_combine.txt processed data file\n",
    "    for line in data : \n",
    "        \n",
    "        # strip end of line characters\n",
    "        line = line.strip()\n",
    "        \n",
    "        # skip empty line\n",
    "        if len(line) < 2 :\n",
    "            continue\n",
    "        \n",
    "        # find pairs in the line\n",
    "        pairs = line.split()\n",
    "        \n",
    "        # initialize the start tag value\n",
    "        prev_tag = \"START\"\n",
    "        \n",
    "        \n",
    "        # iterate over every pair\n",
    "        for pair in pairs : \n",
    "            # split word tag pair\n",
    "            try :\n",
    "                [word,cur_tag] = pair.split(\"_\")\n",
    "            except : \n",
    "                print(pair)\n",
    "                continue\n",
    "                    \n",
    "            [word,cur_tag] = pair.split(\"_\")\n",
    "            \n",
    "            # obtain the dictionary for previous tag\n",
    "            prev_tag_dict = tag_dict.get(prev_tag, {})\n",
    "            \n",
    "            # update the value of this tag, in the dictionary of previous tag\n",
    "            prev_tag_dict[cur_tag] = prev_tag_dict.get(cur_tag,0)+1\n",
    "            \n",
    "            # updtate the complete main dictionary with new values\n",
    "            tag_dict[prev_tag] = prev_tag_dict\n",
    "            prev_tag=cur_tag\n",
    "        # sentence iterated\n",
    "        # increase count\n",
    "        count = count +1\n",
    "        \n",
    "        # print statement for motivation purposes\n",
    "        if count == 10000:\n",
    "            print(count, \" sentences processed\")\n",
    "    # whole file done\n",
    "    print(\"total number of sentences in the text \" , count)\n",
    "    \n",
    "    # now unpickle the tag file\n",
    "    tags_count_file = open(\"pickle/tagscount\",\"rb\")\n",
    "    tags_count_dict = pickle.load(tags_count_file)\n",
    "    \n",
    "    # for this run. add aditional value\n",
    "    tags_count_dict[\"START\"] = count\n",
    "    \n",
    "    # iterate over every tag in the list\n",
    "    for t1 in tag_dict :\n",
    "        \n",
    "        # now for every tag in the sub dictionary updtae the value\n",
    "        for t2 in tag_dict[t1] :\n",
    "            # probability is equal to number of times t2 occurs after t1\n",
    "            # divided by the number of times t1 occurs in the whole train data\n",
    "            # we multiply by 1.0 to get a double value\n",
    "            tag_dict[t1][t2] = (1.0*tag_dict[t1][t2])/ tags_count_dict[t1]\n",
    "    \n",
    "    # now pickle this dictionary\n",
    "    transmission_pickle = open (\"pickle/transmission\", \"wb+\")\n",
    "    pickle.dump(tag_dict,transmission_pickle)\n",
    "    transmission_pickle.close()\n",
    "    \n",
    "    print(\"Function completed succesfully, all glories to HDGACBVSSP, pickle saved for transmission prob\")\n",
    "            \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please UNcomment below block to generate transmission probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get transmission probabilities\n",
    "transmission_mk()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#viterbi algorithm implemention\n",
    "#observed states are words\n",
    "#hidden states are tags\n",
    "\n",
    "def viterbi():\n",
    "    \n",
    "    file = open(\"processed_data_sentence/test_sentence_combine.txt\", \"r\", encoding= \"utf-8\")\n",
    "    \n",
    "    pickled_emission=open(\"pickle/emission\",\"rb\")\n",
    "    emission_dict=pickle.load(pickled_emission)\n",
    "    pickled_emission.close()\n",
    "    \n",
    "    pickled_transition=open(\"pickle/transmission\",\"rb\")\n",
    "    transition_dict=pickle.load(pickled_transition)\n",
    "    pickled_transition.close()\n",
    "    \n",
    "    pickled_tags=open(\"pickle/tagscount\",\"rb\")\n",
    "    tag_dict=pickle.load(pickled_tags)\n",
    "    pickled_tags.close()\n",
    "    \n",
    "    tag_list=[*tag_dict]\n",
    "    \n",
    "    total_tag=tag_list\n",
    "    predicted=0\n",
    "    actual=0\n",
    "    confusion_dict={}\n",
    "    cnt=0\n",
    "    for sentence in file:\n",
    "        \n",
    "        #initialising\n",
    "        #extracting tag list\n",
    "\n",
    "        #storing the the total probability\n",
    "        final_value={}\n",
    "        sentence=sentence.strip()\n",
    "        if len(sentence) <2:\n",
    "            continue\n",
    "        pairs = sentence.split()\n",
    "        wordlis = []\n",
    "        taglis = []\n",
    "\n",
    "\n",
    "        for pair in pairs :\n",
    "            try :\n",
    "                [word,trag] = pair.split(\"_\")\n",
    "            except :     \n",
    "                print(pair)\n",
    "                continue\n",
    "            [word,tag] = pair.split(\"_\")\n",
    "            wordlis.append(word)\n",
    "            taglis.append(tag)\n",
    "            \n",
    "\n",
    "        for tag in tag_list:\n",
    "            \n",
    "            a=emission_dict[tag].get(word,0.00000001)\n",
    "            b=transition_dict[\"START\"].get(tag,0.00000001)\n",
    "\n",
    "            final_value[tag]=a*b\n",
    "            \n",
    "        \n",
    "        final_value_tmp={}\n",
    "        final_value_tmp=final_value\n",
    "        result =[]\n",
    "        for word in wordlis[1:]:\n",
    "            \n",
    "            final_value=final_value_tmp\n",
    "            result.append(max(final_value, key=final_value.get))\n",
    "            \n",
    "            final_value_tmp={}\n",
    "            \n",
    "            for tag_cur in tag_list:\n",
    "                tmp=[]\n",
    "                for tag_prev in tag_list:\n",
    "    #                 a=emission_dict[tag_cur].get(word,0.00000001)\n",
    "    #                 b=transition_dict[tag_prev].get(tag_cur,0.00000001)\n",
    "\n",
    "                    if word in emission_dict[tag_cur]:\n",
    "                        a=emission_dict[tag_cur][word]\n",
    "                    else:\n",
    "                        a=0.00000001\n",
    "                    if tag_prev in transition_dict and tag_cur in transition_dict[tag_prev]:\n",
    "                        b=transition_dict[tag_prev][tag_cur]\n",
    "                    else:\n",
    "                        b=0.00000001\n",
    "\n",
    "                    tmp.append(a*b*final_value[tag_prev])\n",
    "                final_value_tmp[tag_cur]=max(tmp)\n",
    "        result.append(max(final_value, key=final_value.get))\n",
    "        \n",
    "        cnt =cnt + 1\n",
    "        if cnt%10000==0:\n",
    "            print(cnt, \" sentences processed\")\n",
    "            break\n",
    "        \n",
    "#         print(taglis)\n",
    "#         print(result)\n",
    "        actual +=len(result)\n",
    "        predicted +=len(result)\n",
    "        for i in range(len(result)):\n",
    "            \n",
    "            if taglis[i] not in total_tag:\n",
    "                total_tag.append(taglis[i])\n",
    "                \n",
    "            tmp_dict=confusion_dict.get(taglis[i],{})\n",
    "            tmp_dict[result[i]]=tmp_dict.get(result[i],0)+1\n",
    "            confusion_dict[taglis[i]]=tmp_dict\n",
    "            \n",
    "            if taglis[i] != result[i]:\n",
    "                predicted =predicted-1\n",
    "                \n",
    "    print(predicted/actual)\n",
    "    \n",
    "    mat=[[0 for i in range(len(total_tag))] for j in range(len(total_tag))]\n",
    "\n",
    "    for i in range(len(total_tag)):\n",
    "        for j in range(len(total_tag)):\n",
    "            tmp_dict=confusion_dict.get(total_tag[i],{})\n",
    "            mat[i][j]=tmp_dict.get(total_tag[j],mat[i][j])\n",
    "\n",
    "        \n",
    "    np.set_printoptions(threshold=np.inf ,linewidth=np.inf)\n",
    "    # define data\n",
    "    TAG = np.asarray(total_tag)\n",
    "    # save to csv file\n",
    "    np.savetxt('total_tags.csv', TAG.astype(str), fmt='%s', delimiter=',')\n",
    "\n",
    "    np.set_printoptions(threshold=np.inf ,linewidth=np.inf)\n",
    "    # define data\n",
    "    data = np.asarray(mat)\n",
    "    # save to csv file\n",
    "    np.savetxt('confusion_matrix.csv', data.astype(int), fmt='%d', delimiter=',')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000  sentences processed\n",
      "0.8077640944526042\n"
     ]
    }
   ],
   "source": [
    "viterbi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
