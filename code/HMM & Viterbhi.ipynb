{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLease this time, before Anyone make any changes, please let me know, \n",
    "# Since Mam Has given us one more chance, I don't want to miss it\n",
    "# thanks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# used to parse the files\n",
    "import xml.etree.ElementTree as xmltree\n",
    "\n",
    "# for searching the folders\n",
    "from os.path import isdir\n",
    "from os.path import isfile\n",
    "from os import mkdir\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will directly dump the word_tag pairs from give file in the their corresponding word_tag files\n",
    "# in sentence format\n",
    "\n",
    "# so the format is \n",
    "# every line of dest file will be a sentence\n",
    "# we will only consider word tags this time.\n",
    "# and sentences will be stored as word_tag pairs\n",
    "\n",
    "def parseFile_mk(source,dest) :\n",
    "    \n",
    "    # xmlfile is the file to be handled\n",
    "    xmlfile = open (source, \"r\",encoding= \"utf-8\")\n",
    "    # we get the text into a variable\n",
    "    xmlfiletext = xmlfile.read();\n",
    "    #close xmlfile\n",
    "    xmlfile.close()\n",
    "    \n",
    "    # make am xml tree from that text\n",
    "    tree = xmltree.fromstring(xmlfiletext)\n",
    "    \n",
    "    # make list of sentences\n",
    "    slist = tree.iter('s')\n",
    "    \n",
    "    #open dest file with **overwriting** mode\n",
    "    dfile = open (dest[:-4]+\"_sentence.txt\", \"w+\", encoding = \"utf-8\")\n",
    "    \n",
    "    \n",
    "    # put the data in the sfile\n",
    "    for sentence in slist :\n",
    "        # declare a line variable to store the sentence\n",
    "        line = \"\"\n",
    "        # iterate over every variable in sentence\n",
    "        for elem in sentence :\n",
    "            #make sure we only process words for a sentence\n",
    "            if elem.tag != 'w' :\n",
    "                continue \n",
    "            # fetch pos tag of this elem\n",
    "            pos = elem.get('c5')\n",
    "            # if the child doesnot contains c5 then it is of no use for us\n",
    "            if pos is None :\n",
    "                continue\n",
    "                \n",
    "            # upper case all pos tags    \n",
    "            pos = pos.upper()   \n",
    "            \n",
    "            # some where the authors were confused and they assigned two tags\n",
    "            # in such scenarios we are deciding to go with first tag and ignore the second\n",
    "            pos = pos[:3]\n",
    "            \n",
    "            # if I am here it  means pos has c5 tag available so we need to find the text of it.\n",
    "            txt = elem.text\n",
    "            if txt is None : \n",
    "                continue\n",
    "            txt = txt.strip()\n",
    "            # append in the sentence this word\n",
    "            line =   line +      txt + \"_\"+ pos        +\" \"  # extra space added between terms \n",
    "                                                             # to just make more clear\n",
    "        # for word loop finshes\n",
    "        # write the sentence to the file\n",
    "        dfile.write(line+\"\\n\")\t\n",
    "    # for sentence loop finished \n",
    "    #close file\n",
    "    dfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function\n",
    "# this will fetch all the files and generate corresponding files\n",
    "# this takes source folder and \n",
    "def makeSentences_mk(source, dest) :\n",
    "\n",
    "    #check if the source is a file\n",
    "    if isfile(source):\n",
    "        # if yes, then process the file \n",
    "        # if destination is not there , the function will create it, and if it is there, it will overrite it.\n",
    "        parseFile_mk(source,dest)\n",
    "    else:\n",
    "        # so the source is a folder, we need to make corresponding folder in the dest \n",
    "        #check if dest has the folder\n",
    "        if not isdir(dest):\n",
    "            mkdir(dest)\n",
    "        \n",
    "        # now enumerate all the items in the source directory    \n",
    "        for item in listdir(source):\n",
    "            makeSentences_mk(source+'/'+item,dest+'/'+item)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Uncomment & Run below Cell to Process Files, \n",
    "## I do processed my files so I have commented this to avoid repeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this segment is used to make the files process all togethor in their own files\n",
    "\n",
    "# source = \"given_data\"\n",
    "# dest = \"processed_data\"\n",
    "# makeSentences_mk(source, dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Function to Combine all the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collector function, this will dump all sentences into the destination file\n",
    "def dump_mk(source,dest):\n",
    "\n",
    "    sfile = open(source, \"r\", encoding= \"utf-8\")\n",
    "    dfile = open(dest, \"a\", encoding= \"utf-8\")\n",
    "\n",
    "    # read entire sfile line by line, yes this is the key\n",
    "    # Possible Error Point : in our Previous Implementation I was reading It all at same time, this was problem ,\n",
    "        # whole file became a single sentence\n",
    "    # and then dump that  into the dfile as itis.\n",
    "    for line in sfile :\n",
    "        dfile.write(line + \"\\n\")\n",
    "\n",
    "    sfile.close()\n",
    "    dfile.close()\n",
    "\n",
    "# this iterates and finds all the files and stores them into a single file \n",
    "# using dumping function\n",
    "def combine_mk(source, dest) :\n",
    "\n",
    "    #check if the source is a file\n",
    "    if isfile(source):\n",
    "        # if yes, then process the file \n",
    "        # if destination is not there , the function will create it, and if it is there, it will overrite it.\n",
    "        dump_mk(source,dest)\n",
    "    else:\n",
    "        # so the source is a folder, \n",
    "        # now enumerate all the items in the source directory  \n",
    "        # and take appropriate actions  \n",
    "        for item in listdir(source):\n",
    "            combine_mk(source+'/'+item,dest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Uncomment Below TWO Cells to Combine the Test & Train Data\n",
    "## I did it thus I have left them commented\n",
    "\n",
    "If you need anyhelp Sritam, just whatsapp me .. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # used to combine all the processed files for both train & test data sets \n",
    "# # together one to each other in their respective files\n",
    "\n",
    "# # Combine Test Data\n",
    "# s=\"processed_data/test_corpus\"\n",
    "# d = \"processed_data/test_combine.txt\"\n",
    "# dfile = open(d,\"w+\", encoding=\"utf-8\")\n",
    "# dfile.close()\n",
    "\n",
    "# combine_mk(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine Train Data\n",
    "# s=\"processed_data/train_corpus\"\n",
    "# d = \"processed_data/train_combine.txt\"\n",
    "# dfile = open(d,\"w+\", encoding=\"utf-8\")\n",
    "# dfile.close()\n",
    "\n",
    "# combine_mk(s,d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Emission Probabilities\n",
    "\n",
    "I saw many videos on youtube with many examples, now I think this is first step we should do. \n",
    "And if we can do this nicely, this will give us COnfidence as well :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Does Emission Probability means?\n",
    "Make this thing clear, so that implementation is not wrong.\n",
    "\n",
    "Hidden states are tags \n",
    "And observations are words... \n",
    "\n",
    "So emission probability means, for word we calculate the chance that given tag, what is probability this word will occur \n",
    "\n",
    "Emission P of word w with tag t = Count of word_tag pairs with word = w and tag = t divided by count of word_tag pairs with tag equal to t no matter what word is....\n",
    "\n",
    "### So far so good ... \n",
    "\n",
    "Obviously this makes things clear.\n",
    "\n",
    "I will make a dictionary counting appearnace of every tag , this will give me count of word_tag pairs with tag t for every tag t\n",
    "\n",
    "Or better make dictionary of dictionary\n",
    "Dict with keys as tags, and second dictionary with keys as words ? how's this ... seems fine right... this will give us all the data we need to go \n",
    "\n",
    "Finally to save it I will pickle the struct, and will see if I need to do anything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the emission probabilty calculator functtion\n",
    "\n",
    "def emission_mk():\n",
    "    # obviosly source file is \n",
    "    data = open(\"processed_data/train_combine.txt\", \"r\", encoding=\"utf-8\")\n",
    "    \n",
    "    # make dictonary for tags as keys\n",
    "    tag_dict = {}\n",
    "    \n",
    "    # now read every line one by one of this file\n",
    "    for line in data :\n",
    "        \n",
    "        # split line into word_tag pairs with split\n",
    "        pairs = line.split()\n",
    "        \n",
    "        # by pass empty line\n",
    "        if len(pairs) < 1: \n",
    "            continue\n",
    "        \n",
    "        # iterate over pairs\n",
    "        for pair in pairs :\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
